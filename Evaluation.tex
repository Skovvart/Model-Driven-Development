\section{Evaluation}
\subsection{Method}

We evaluate and test the implementation of our framework for behavior-driven development by means of a case study of another project created with accompanying contracts which we can rewrite to behaviors in natural English. On the basis of these behaviors we can use our framework to generate low level code contracts and then compare the results to the original contracts included in the project.
 
This yields the following plan for our evaluation. Using a subset of an already existing system with code contracts, we...

\begin{itemize}
	\item Convert contracts into extended Gherkin
	\item Run our Salad framework with Lettuce transformation rules on them to obtain Tomato code
	\item Compare the Tomato code to the originial JML contracts
\end{itemize}

We have chosen to use the V\'{o}t\'{a}il project \cite{votailCochranKiniry}. V\'{o}t\'{a}il is an open-source implementation that models the electoral system in Ireland specified in Business Object Notation (BON) \cite{bonmethod} and later refined into JML.
 
We believe this is a suitable project to use as it is limited in size and the total set of contracts is fairly diverse in terms of complexity (simple/advanced) and type (invariants, preconditions, postconditions etc).

The success of our evaluation depends on the level of reproducibility that we achieve, i.e. how well can our generated contracts capture the same semantics as their original counterparts included in the project while at the same time being generated from high-level behaviors whose semantics are understood by a non-programmer.
 
While we will not perform any objective tests to estimate how suited a behaviour described in natural English is in a business context (we will probably give our opinion), we will evaluate the generated result. We do this both qualitatively (is it precise? does it cover the original contract? are they identical?) and quantitatively (how precise? to what degree is it covered?)
 
With respect to the quantitative measures, we will state our evaluations in unit of percentage (how many of the generated contracts are adequately resembling their original counterpart to be considered successful) and set a threshold for overall success of 80\%.